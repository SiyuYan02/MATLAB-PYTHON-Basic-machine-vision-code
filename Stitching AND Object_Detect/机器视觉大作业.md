<div align='center' ><font size='6'>机器视觉大作业</font></div>
<div align='right'>智能科学与技术 1913416 阎丝雨</div>
------

## 一、**全景图拼接**

本代码实现了将传入的两张图片进行拼接，并对拼接裂缝进行了消除处理。

步骤：输入图片---(柱形变换)---(去黑边)---关键点检测---特征匹配---图像配准---图像融合与融合边界处理

结果：![image](https://github.com/SiyuYan02/imgs/blob/main/vision-imgs/001f6e509df8cc227ac675be7c34e46.png)

![result_33](C:\Users\yan\PycharmProjects\pythonProject\result_33.jpg)

调用示例：

```python
#输入待拼接图片
img_1 = cv2.imread("result_40.png")
# img_1_=cylindrical_projection(img_1,1000)
# img_1_=cutblackedges(img_1_)

img_2 = cv2.imread("result_39.png")
# img_2_=cylindrical_projection(img_2,500)
# img_2_=cutblackedges(img_2)

#拼接结果
result = stitch(images = [img_1,img_2], showMatches=False)
result=cutblackedges(result)
cv2.imwrite('result_42.png',result)
pltshowim('result',result)

```

------

*下面结合代码分析处理过程：*

### **1、特征提取（关键点检测）**

------

利用sift特征探测器来检测出**两幅图片的sift关键点**。

对于给定的图片，若要实现把它们拼接起来创建一个全景场景需要两个图像都需要共享一些共同的区域。第一步是提取关键点的坐标和特征向量。

提取关键点的坐标和特征向量通过detectAndCompute（）函数完成。在使用此函数之前需要将图片转换为灰度图并且建立一个SIFT算子作为检测器。

```python
def detectAndDescribe(image):
    
    #输入：
    #image:待处理图片
    #输出：
    #关键点集，及对应的特征向量
    
    # 将彩色图片转换成灰度图
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # 建立SIFT算子
    sift = cv2.xfeatures2d.SIFT_create()

    # 计算出图像的关键点和sift特征向量，kpA表示图像A关键点的坐标,features表示特征向量
    (kps, features) = sift.detectAndCompute(image, None)

    # 将结果转换成NumPy数组
    kps = np.float32([kp.pt for kp in kps])

    # 返回关键点集，及对应的特征向量
    return (kps, features)
```



### 2、特征匹配 

------

易知两个图像中都有大量的特征，若要做到**图像拼接需要将图像相匹配的特征点结合**，此步骤实现了特征点的匹配。

每个特征点都有三个描述信息（尺度空间，模长，方向）。然后是进行匹配我们采用关键点特征向量的欧式距离来作为两幅图像中关键点的相似性判定度量。

使用KNN匹配法将特征点找到每个点的k个最佳匹配，设定k值为2。

为了确保KNN返回的特征具有良好的可比性，实行迭代KNN返回的每个对并执行距离测试。对于每对特征（f1，f2），在这两个关键点中，如果最近的距离除以次近的距离少于某个比例阈值，则接受这一对匹配点，否则，我们将它丢弃。

得到匹配后的特征点后需要找到基于匹配点将2个图像拼接在一起的变换矩阵。这种转换称为Homography matrix（单应性矩阵）。

```python
def matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh):
    
    #输入：
    #kpsA, kpsB, featuresA, featuresB:A,B图片的特征点与特征向量
    #ratio:比例阈值
    #reprojThresh:距离阈值
    #输出：
    #匹配的特征点对、视角变换矩阵
    
    # 建立暴力匹配器，使用默认的欧氏距离
    matcher = cv2.BFMatcher()

    # 使用KNN检测来自A、B图的SIFT特征匹配对，K=2
    rawMatches = matcher.knnMatch(featuresA, featuresB, 2)

    matches = []
    for m in rawMatches:
        # 当最近距离跟次近距离的比值小于ratio值时，保留此匹配对
        if len(m) == 2 and m[0].distance < m[1].distance * ratio:
            # 存储两个点在featuresA, featuresB中的索引值
            matches.append((m[0].trainIdx, m[0].queryIdx))

    # 当筛选后的匹配对大于4时，计算视角变换矩阵
    if len(matches) > 4:
        # 获取匹配对的点坐标
        ptsA = np.float32([kpsA[i] for (_, i) in matches])
        ptsB = np.float32([kpsB[i] for (i, _) in matches])

        # 计算视角变换矩阵
        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)

        # 返回结果
        return (matches, H, status)

    # 如果匹配对小于4时，返回None
    return None
```



### 3、图像配准

------

获得特征点匹配对之后，遍历两张图片，将对应的特征点对连接在一起，得到可视化的特征匹配结果。

```python
def drawMatches(imageA, imageB, kpsA, kpsB, matches, status):
    
    #输入：
    #imageA, imageB, kpsA, kpsB:图片A,B，特征点集
    #matches:匹配的特征点对
    #status:掩膜
    #输出：
    #可视化的特征匹配结果
    
    # 初始化可视化图片，将A、B图左右连接到一起
    (hA, wA) = imageA.shape[:2]
    (hB, wB) = imageB.shape[:2]
    vis = np.zeros((max(hA, hB), wA + wB, 3), dtype="uint8")
    vis[0:hA, 0:wA] = imageA
    vis[0:hB, wA:] = imageB

    # 联合遍历，画出匹配对
    for ((trainIdx, queryIdx), s) in zip(matches, status):
        # 当点对匹配成功时，画到可视化图上
        if s == 1:
            # 画出匹配对
            ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))
            ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))
            cv2.line(vis, ptA, ptB, (0, 255, 0), 1)

    # 返回可视化结果
    return vis
```



### 4、图像融合与融合边界处理

------

此函数首先调用了detectAndDescribe()函数得到图片的关键点集，及对应的特征向量。下一步调用了matchKeypoints()函数获得得到 estimated Homography，我们需要将其中一个图像变换到一个共同的平面。
在这里，我们将对其中一个图像应用透视变换。基本上，透视变换可以组合一个或多个操作，例如旋转，缩放，平移或剪切。这个想法是转换其中一个图像，使两个图像合并为一个。为此，我们可以使用warpPerspective（）函数。它采用图像和homography作为输入。然后，它根据homography将源图像变换到目的平面上。

对于一组图像数据集中的两幅图像，通过寻找一种空间变换把一幅图像映射到另一幅图像，使得两图中对应于空间同一位置的点一一对应起来，从而达到信息融合的目的。

此外，发现在图像拼接部分有较明显的拼接裂缝，因此采用加权的方法来消除缝隙，由于拼接的过程是将B覆盖在图片A上，因此在裂缝左面将图片B与A叠加加权，越靠近裂缝部分取A的比例越大。

```python
def stitch(images, ratio=0.75, reprojThresh=4.0,showMatches=False):
    
    	#输入：
        #images:待拼接图片
        #ratio:设定的比例阈值
        #reprojThresh:设定的距离阈值
        #输出：
        #拼接后的图片

        #获取输入图片
        (imageB, imageA) = images
        #检测A、B图片的SIFT关键特征点，并计算特征描述子
        (kpsA, featuresA) = detectAndDescribe(imageA)
        (kpsB, featuresB) = detectAndDescribe(imageB)

        # 匹配两张图片的所有特征点，返回匹配结果
        M = matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh)

        # 如果返回结果为空，没有匹配成功的特征点，退出算法
        if M is None:
            return None

        # 否则，提取匹配结果
        # H是3x3视角变换矩阵
        (matches, H, status) = M

        # 将图片A进行视角变换，result是变换后图片
        result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], max(imageA.shape[0],imageB.shape[0])))
        row_B, col_B, dB = imageB.shape
        for i in range(row_B):
            for j in range(col_B):
                if result[i][j][0] == 0 and result[i][j][1] == 0 and result[i][j][2] == 0:
                    result[i][j] = imageB[i][j]
                else:
                    x = j / col_B
                    a=x**9  # A的比例
                    b = 1 - a  # B的比例
                    result[i][j] = a * result[i][j] + b * imageB[i][j]
        # 将图片B传入result图片最左端

        # result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB
        # 检测是否需要显示图片匹配
        if showMatches:
            # 生成匹配图片
            vis = drawMatches(imageA, imageB, kpsA, kpsB, matches, status)
            # 返回结果
            return (result, vis)

        # 返回匹配结果
        return result
```



### 5、输出函数、柱形变换函数、去黑边函数

------

输入采用imread()函数，若用imshow()输出，图片过大会显示不全，因此选择plt输出，此输出函数实现了从imshow到plt输出的转化

```python
def pltshowim(title, img):
    tmp = img[:, :, [2, 1, 0]]
    plt.figure()
    plt.imshow(tmp)
    plt.title(title)
    plt.show()
```

在拼接角度变化较大的图片时，如果直接拼接会造成图片的拉伸变形，于是需要在处理图片之前对图像进行柱形变换处理

```python
def cylindrical_projection(img, f):
    rows = img.shape[0]
    cols = img.shape[1]

    # f = cols / (2 * math.tan(np.pi / 8))

    blank = np.zeros_like(img)
    center_x = int(cols / 2)
    center_y = int(rows / 2)

    for y in range(rows):
        for x in range(cols):
            theta = math.atan((x - center_x) / f)
            point_x = int(f * math.tan((x - center_x) / f) + center_x)
            point_y = int((y - center_y) / math.cos(theta) + center_y)

            if point_x >= cols or point_x < 0 or point_y >= rows or point_y < 0:
                pass
            else:
                blank[y, x, :] = img[point_y, point_x, :]
    return blank
```

在部分图像拼接以及柱形变换后会产生黑边（在面对不同的黑边类型选择不同的才接黑边代码段），于是就有了下面的去除黑边函数

```python
def cutblackedges(result):
    row_R, col_R, dR = result.shape
    global edges_xmin, edges_xmax
    for j in range(col_R):
        if result[int(row_R/2)][j][0] != 0 and result[int(row_R/2)][j][1] != 0 and result[int(row_R/2)][j][2] != 0:
            edges_xmin = j
            break
    for j in reversed(range(col_R)):
        if result[int(row_R/2)][j][0] != 0 and result[int(row_R/2)][j][1] != 0 and result[int(row_R/2)][j][2] != 0:
            edges_xmax = j
            break
    row_R, col_R, dR = result.shape
    for j in range(col_R):
        if result[0][j][0] == 0 and result[0][j][1] == 0 and result[0][j][2] == 0:
            edges_x = j
            break
    # for j in range(col_R):
    #     if result[row_R - 1][j][0] == 0 and result[row_R - 1][j][1] == 0 and result[row_R - 1][j][2] == 0:
    #         edges_x2 = j
    #         break
    # edges_x = min(edges_xx, edges_x2)
    # gray2 = result[:, :edges_x-1]
    res = result[:, :edges_xmax - 1]
    return res
```

------

##### 结果分析：尽管在复杂图像拼接中出现了亮度不均匀的情况，最后还算是较好达到了了图像拼接的目的

## 二、**目标识别**

代码实现了基于训练好的图像分类网络（基于ImageNet）给出特定检测目标的包围框（以检测树木为例）

步骤：形成候选区域

检测结果：![result_47](C:\Users\yan\PycharmProjects\pythonProject\result_47.jpg)

![result_44](C:\Users\yan\PycharmProjects\pythonProject\result_44.jpg)

调用实例：

```python
#载入图片
img = cv2.imread("result_33.jpg")
candidates = GetRect(img)  # 获取候选矩形框
res_img = GetClassification(img, candidates)  # 返回框选后的结果图像
ShowImg_plt('$Object \quad Detection$', res_img)
cv2.imwrite('result_36.jpg', res_img)
```

------

*下面结合代码分析处理过程：*

### 一、形成候选区域

------

候选区域的形成采用SelectiveSearch算法实现。调用selective_search()函数得到大概2000个候选区域，通过限制候选区矩形框的大小排除掉一些大小不合适的矩形框。

```python
def GetRect(img):
    #输入:
    #img: 输入待处理图像图像
    #输出
    #经过selectivesearch检测并进行删减之后的矩形框

    # 用selectivesearch进行候选区选择
    img_lbl, rects = selectivesearch.selective_search(img, scale=500, sigma=0.9, min_size=10)

    candidates = set()
    for r in rects:
        x, y, w, h = r['rect']
        # excluding regions smaller than 2000 pixels
        if r['size'] < 2000:
            continue
        # excluding same rectangle (with different segments)
        if r['rect'] in candidates:
            continue
        # 排除长宽比不合适的矩形
        if w / h > 5 or h / w > 5:
            continue
        candidates.add(r['rect'])

    return candidates
```



### 二、**特征提取**

------

在得到了候选框后我们想要获取其中的特征，在获取特征之前需要先对图像大小固定。

将各个候选区域通过predict()函数后，得到分类、描述、概率。

此函数中经历了获取候选区域的4096维特征之后送入SVM训练器，得到此候选区域为不同分类的概率，选取最大的一个概率并返回。

```python
img = img.copy()
model = ResNet50(weights='imagenet')  # 载入ResNet50网络模型，并使用在ImageNet ILSVRC比赛中已经训练好的权重
target_size = (224, 224)  # ResNet50的输入大小固定为(224,224)，其他大小会报错
top_n = 1  # 只输出最高概率对应的一类

for x, y, w, h in candidates:
    sub_img = img[y:y + h, x:x + w, :]  # 获得矩形框内的图像
    pred = predict(model, target_size, top_n, sub_img)
```

```python
def predict(model, target_size, top_n, img):

    # 输入：
    # model：分类模型  model = ResNet50(weights='imagenet')
    # target_size：输入图像需要resize的大小 target_size = (224, 224)
    # top_n:输出概率最高的几个类 top_n=1
    # img：待预测图像 img = cv2.imread("1.jpg")
    # 输出：
    # 图像预测结果的元组列表（类、描述、概率）
    # 调整数组形状：

    if img.shape[0] != target_size[0] or img.shape[1] != target_size[1]:
        img = cv2.resize(img, target_size)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)

    # 预测
    pred = model.predict(img)
    return decode_predictions(pred, top=top_n)[0]
```



### 三、非极大值抑制

------

此时图片中还有过多的候选区域，尤其是很多区域相互重叠面积过大影响最终的判断结果，因此需进行非极大值抑制将重复面积过大的候选框排除。

IOU为衡量两个矩形框重叠程度的指标。

```python
def None_Max_Supression(bounding_boxes, confidence_score, classifications, IOU_thred):
    # 输入：
    # bounding_boxes: 矩形框的尺寸
    # confidence_score: 设定的识别置信度阈值
    # classifications: 对应预测出的阈值
    # IOU_thred: 函数中设定好的IOU置信度阈值
    # 输出：
    # 选中的矩形框的大小、置信度、种类

    # 将list转为array
    boxes = np.array(bounding_boxes)
    score = np.array(confidence_score)

    # 获得四个坐标点分别对应的array
    start_x = boxes[:, 0]
    start_y = boxes[:, 1]
    end_x = boxes[:, 2]
    end_y = boxes[:, 3]

    # 最终返回的经过非极大值抑制之后的最终筛选结果
    picked_boxes = []
    picked_score = []
    picked_class = []

    # 计算每一个矩形框的面积
    areas = (end_x - start_x + 1) * (end_y - start_y + 1)

    # 选择当前候选矩形框中具有最大置信度的矩形框
    order = np.argsort(score)

    # 开始迭代筛选
    while order.size > 0:
        index = order[-1]

        # 把最大置信度的矩形放到最终的结果中
        picked_boxes.append(bounding_boxes[index])
        picked_score.append(confidence_score[index])
        picked_class.append(classifications[index])

        # 计算每一个矩形对应于最大置信度矩形的重叠区域
        x1 = np.maximum(start_x[index], start_x[order[:-1]])
        x2 = np.minimum(end_x[index], end_x[order[:-1]])
        y1 = np.maximum(start_y[index], start_y[order[:-1]])
        y2 = np.minimum(end_y[index], end_y[order[:-1]])

        # 计算相交框的面积,注意矩形框不相交时w或h算出来会是负数，用0代替
        w = np.maximum(0.0, x2 - x1 + 1)
        h = np.maximum(0.0, y2 - y1 + 1)
        intersection = w * h

        # 计算相应的IOU
        ratio = intersection / (areas[index] + areas[order[:-1]] - intersection)

        # 删除小于设定IOU的矩形
        left = np.where(ratio < IOU_thred)
        order = order[left]

    return picked_boxes, picked_score, picked_class
```





### 四、获得分类

------

此函数首先实现了特征提取的步骤，获得每个区域的分类、描述、概率。由于要求检测出指定类别，因此排除掉不属于指定类别的区域框，初步排除掉置信度过低的区域，使得最后结果中的分类都有较高的准确率。之后进行极大值抑制排除掉重合面积过大的矩形框，经过这两部处理后最后得到的区域有较高的准确率以及独立性。最后进行可视化处理，显示每个框内识别出的分类。

```python
def GetClassification(img, candidates, special_class='African_elephant', confidence=0.1, IOU_thred=0.2):

    # 输入：
    # IOU_thred: 进行非极大抑制时的阈值，目的是去除重合面积较大的矩形框
    # confidence: 识别置信度阈值
    # special_class: 需要检测出的特定类别
    # img: 输入待处理的图像
    # candidates: 经过选择性搜索以及前处理获得的候选矩形框
    # 输出：
    # 每一个矩形的大小、对应概率以及分类


    bounding_boxes = []  # 容纳被选中的矩形框的尺寸的list
    confidence_score = []  # 容纳检测出的物体对应预测值的置信度的list
    classifications = []  # 容纳识别出的类别所构成的list

    # 调用示例：
    img = img.copy()
    model = ResNet50(weights='imagenet')  # 载入ResNet50网络模型，并使用在ImageNet ILSVRC比赛中已经训练好的权重
    target_size = (224, 224)  # ResNet50的输入大小固定为(224,224)，其他大小会报错
    top_n = 1  # 只输出最高概率对应的一类

    for x, y, w, h in candidates:
        sub_img = img[y:y + h, x:x + w, :]  # 获得矩形框内的图像
        pred = predict(model, target_size, top_n, sub_img)

        # 找出需要检测出的特定类别
        classification = pred[0][1]
        if special_class is not None and special_class != classification:
            continue

        # 去除掉置信度过低的矩形框
        if pred[0][2] > confidence:
            bounding_boxes.append([x, y, x + w, y + h])
            confidence_score.append(pred[0][2])
            classifications.append(classification)

    # 非极大值抑制
    picked_boxes, picked_score, picked_class = None_Max_Supression(bounding_boxes, confidence_score,

    for idx in range(len()):
        x1, y1, x2, y2 = picked_boxes[idx]
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 1, cv2.LINE_AA)
        img = cv2.putText(img, picked_class[idx], (x1 - 10, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)

    return img
```

------

##### 结果分析：最后复杂图片达到了5/7的识别率，简单图片达到了4/5的识别率 ，将其余未识别出的部分截图出来进入resnet检测后，发现可以识别出为电脑，但是准确率到了0.05，所以此程序没有检测出也可以原谅